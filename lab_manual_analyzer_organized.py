#!/usr/bin/env python3
"""
Lab Manual Analyzer - Version LaTeX STRICTE avec double validation JSON
AUCUN FALLBACK - √âchec propre si probl√®me
USAGE M√âDICAL - Fiabilit√© critique avec Gemini 2.0 Flash + correction automatique
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import time
import re
import math

# Google Cloud imports
from google.cloud import documentai
import google.generativeai as genai
from google.oauth2 import service_account

# Import du g√©n√©rateur LaTeX strict et d√©chiffreur
from latex_generator import LatexSynthesisGenerator
from pdf_decryptor import decrypt_pdf

# Configuration logging stricte
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('lab_analysis.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class LabManualAnalyzerStrict:
    """Analyseur STRICT pour mat√©riel m√©dical avec double validation JSON"""
    
    def __init__(self, config_path: str = "config.json"):
        """Initialise l'analyseur avec v√©rifications strictes"""
        logger.info("üè• INITIALISATION ANALYSEUR M√âDICAL STRICT")
        
        self.config = self.load_config(config_path)
        self.setup_google_apis()
        self.setup_output_directories()
        
        # Limite Document AI stricte
        self.max_pages_per_request = 15
        logger.info(f"üìÑ Limite Document AI: {self.max_pages_per_request} pages par requ√™te")
        
        # Initialiser le g√©n√©rateur LaTeX STRICT
        try:
            self.latex_generator = LatexSynthesisGenerator()
            logger.info("‚úÖ G√©n√©rateur LaTeX m√©dical initialis√© et v√©rifi√©")
        except Exception as e:
            logger.error(f"‚ùå ERREUR CRITIQUE: G√©n√©rateur LaTeX non op√©rationnel")
            raise RuntimeError(f"Impossible d'initialiser le g√©n√©rateur LaTeX: {e}")
    
    def load_config(self, config_path: str) -> Dict:
        """Charge et valide la configuration de mani√®re stricte"""
        if not Path(config_path).exists():
            raise FileNotFoundError(f"‚ùå Fichier de configuration manquant: {config_path}")
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # Validation stricte de la configuration
            required_sections = {
                "google_cloud": ["project_id", "location", "processor_id", "credentials_path"],
                "gemini": ["api_key", "model"],
                "analysis": ["delay_between_requests"]
            }
            
            for section, keys in required_sections.items():
                if section not in config:
                    raise ValueError(f"‚ùå Section manquante dans config: {section}")
                
                for key in keys:
                    if key not in config[section] or not config[section][key]:
                        raise ValueError(f"‚ùå Cl√© manquante ou vide: {section}.{key}")
            
            # V√©rifier le fichier de credentials
            creds_path = Path(config["google_cloud"]["credentials_path"])
            if not creds_path.exists():
                raise FileNotFoundError(f"‚ùå Fichier credentials manquant: {creds_path}")
            
            logger.info("‚úÖ Configuration valid√©e")
            return config
            
        except json.JSONDecodeError as e:
            raise ValueError(f"‚ùå Configuration JSON invalide: {e}")
    
    def setup_output_directories(self):
        """Cr√©e la structure de dossiers avec v√©rifications"""
        self.manuels_dir = Path("manuels")
        self.syntheses_dir = self.manuels_dir / "syntheses"
        self.temp_dir = Path("temp")
        
        try:
            for directory in [self.manuels_dir, self.syntheses_dir, self.temp_dir]:
                directory.mkdir(exist_ok=True)
                
                # V√©rifier les permissions d'√©criture
                test_file = directory / "test_write_permission.tmp"
                test_file.write_text("test")
                test_file.unlink()
            
            logger.info("‚úÖ Structure de dossiers valid√©e avec permissions d'√©criture")
            
        except Exception as e:
            raise RuntimeError(f"‚ùå Impossible de cr√©er/acc√©der aux dossiers: {e}")
    
    def setup_google_apis(self):
        """Configure les APIs Google Cloud avec v√©rifications strictes"""
        try:
            credentials = service_account.Credentials.from_service_account_file(
                self.config["google_cloud"]["credentials_path"]
            )
            
            region = self.config["google_cloud"]["location"]
            if region == "eu":
                client_options = {"api_endpoint": "eu-documentai.googleapis.com"}
                self.doc_ai_client = documentai.DocumentProcessorServiceClient(
                    credentials=credentials,
                    client_options=client_options
                )
            else:
                self.doc_ai_client = documentai.DocumentProcessorServiceClient(
                    credentials=credentials
                )
            
            self.processor_name = f"projects/{self.config['google_cloud']['project_id']}/locations/{self.config['google_cloud']['location']}/processors/{self.config['google_cloud']['processor_id']}"
            
            # Configuration Gemini stricte
            genai.configure(api_key=self.config["gemini"]["api_key"])
            self.gemini_model = genai.GenerativeModel(self.config["gemini"]["model"])
            
            # Essayer d'utiliser Gemini 2.0 Flash si disponible
            try:
                self.advanced_model = genai.GenerativeModel("gemini-2.0-flash-exp")
                logger.info("‚úÖ Gemini 2.0 Flash d√©tect√© et disponible")
            except:
                self.advanced_model = self.gemini_model
                logger.info("üí° Utilisation du mod√®le Gemini configur√©")
            
            # Test de connexion obligatoire
            self.test_api_connections()
            
            logger.info("‚úÖ APIs Google Cloud configur√©es et test√©es")
            
        except Exception as e:
            raise RuntimeError(f"‚ùå √âchec configuration APIs: {e}")
    
    def test_api_connections(self):
        """Test obligatoire des connexions APIs"""
        try:
            # Test Document AI
            parent = f"projects/{self.config['google_cloud']['project_id']}/locations/{self.config['google_cloud']['location']}"
            request = documentai.ListProcessorsRequest(parent=parent)
            processors = list(self.doc_ai_client.list_processors(request=request))
            
            # V√©rifier que notre processeur existe
            processor_id = self.config["google_cloud"]["processor_id"]
            processor_found = any(processor_id in p.name for p in processors)
            
            if not processor_found:
                raise RuntimeError(f"‚ùå Processeur Document AI non trouv√©: {processor_id}")
            
            # Test Gemini simple
            test_response = self.gemini_model.generate_content("Test de connexion - r√©pondez 'OK'")
            if not test_response.text:
                raise RuntimeError("‚ùå Gemini ne r√©pond pas correctement")
            
            logger.info("‚úÖ Connexions APIs test√©es avec succ√®s")
            
        except Exception as e:
            raise RuntimeError(f"‚ùå Test de connexion √©chou√©: {e}")
    
    def prepare_pdf(self, pdf_path: Path) -> Path:
        """Pr√©pare le PDF avec v√©rifications strictes"""
        if not pdf_path.exists():
            raise FileNotFoundError(f"‚ùå PDF non trouv√©: {pdf_path}")
        
        if pdf_path.stat().st_size == 0:
            raise ValueError(f"‚ùå PDF vide: {pdf_path}")
        
        try:
            import PyPDF2
            
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                if pdf_reader.is_encrypted:
                    logger.info(f"üîí PDF chiffr√© d√©tect√©: {pdf_path.name}")
                    decrypted_path = self.temp_dir / f"decrypted_{pdf_path.name}"
                    
                    if not decrypt_pdf(pdf_path, decrypted_path):
                        raise RuntimeError(f"‚ùå Impossible de d√©chiffrer: {pdf_path}")
                    
                    logger.info(f"üîì PDF d√©chiffr√©: {decrypted_path}")
                    return decrypted_path
                else:
                    logger.info(f"üìÑ PDF non chiffr√©: {pdf_path.name}")
                    return pdf_path
                    
        except ImportError:
            raise RuntimeError("‚ùå PyPDF2 non install√© - requis pour validation PDF")
        except Exception as e:
            raise RuntimeError(f"‚ùå Erreur pr√©paration PDF: {e}")
    
    def split_pdf_15pages(self, pdf_path: Path) -> List[Path]:
        """Divise PDF en chunks stricts de 15 pages maximum"""
        try:
            import PyPDF2
            
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                total_pages = len(pdf_reader.pages)
                
                if total_pages == 0:
                    raise ValueError("‚ùå PDF sans pages")
                
                logger.info(f"üìä PDF: {total_pages} pages √† traiter")
                
                if total_pages <= self.max_pages_per_request:
                    logger.info("üìÑ PDF petit - traitement direct")
                    return [pdf_path]
                
                # D√©coupage strict
                chunks_dir = self.temp_dir / f"{pdf_path.stem}_chunks"
                chunks_dir.mkdir(exist_ok=True)
                
                chunks = []
                pages_per_chunk = self.max_pages_per_request
                num_chunks = math.ceil(total_pages / pages_per_chunk)
                
                logger.info(f"‚úÇÔ∏è D√©coupage en {num_chunks} chunks de {pages_per_chunk} pages max")
                
                for i in range(num_chunks):
                    start_page = i * pages_per_chunk
                    end_page = min(start_page + pages_per_chunk, total_pages)
                    
                    pdf_writer = PyPDF2.PdfWriter()
                    
                    for page_num in range(start_page, end_page):
                        pdf_writer.add_page(pdf_reader.pages[page_num])
                    
                    chunk_path = chunks_dir / f"chunk_{i+1:02d}_p{start_page+1}-{end_page}.pdf"
                    
                    with open(chunk_path, 'wb') as output_file:
                        pdf_writer.write(output_file)
                    
                    # V√©rification chunk cr√©√©
                    if not chunk_path.exists() or chunk_path.stat().st_size == 0:
                        raise RuntimeError(f"‚ùå √âchec cr√©ation chunk: {chunk_path}")
                    
                    chunks.append(chunk_path)
                    logger.info(f"‚úÖ Chunk {i+1}: {end_page - start_page} pages")
                
                return chunks
                
        except Exception as e:
            raise RuntimeError(f"‚ùå √âchec d√©coupage PDF: {e}")
    
    def extract_text_safe(self, pdf_path: Path, max_retries: int = 2) -> Dict:
        """Extraction de texte avec validation stricte"""
        for attempt in range(max_retries):
            try:
                logger.info(f"üîç Extraction: {pdf_path.name} (tentative {attempt + 1})")
                
                with open(pdf_path, "rb") as pdf_file:
                    pdf_content = pdf_file.read()
                
                file_size_mb = len(pdf_content) / (1024 * 1024)
                logger.info(f"üìä Taille: {file_size_mb:.1f} MB")
                
                # Limite stricte de taille
                if len(pdf_content) > 20 * 1024 * 1024:
                    raise ValueError("‚ùå Fichier trop volumineux (>20MB)")
                
                if len(pdf_content) == 0:
                    raise ValueError("‚ùå Fichier vide")
                
                raw_document = documentai.RawDocument(
                    content=pdf_content,
                    mime_type="application/pdf"
                )
                
                request = documentai.ProcessRequest(
                    name=self.processor_name,
                    raw_document=raw_document
                )
                
                result = self.doc_ai_client.process_document(request=request)
                document = result.document
                
                # Validation stricte du r√©sultat
                if not document or not document.text:
                    raise ValueError("‚ùå Document AI n'a extrait aucun texte")
                
                char_count = len(document.text)
                page_count = len(document.pages) if document.pages else 0
                
                # Validation minimale du contenu
                if char_count < 100:  # Trop peu de texte = probl√®me
                    raise ValueError(f"‚ùå Texte extrait insuffisant: {char_count} caract√®res")
                
                logger.info(f"‚úÖ Extraction r√©ussie: {char_count:,} caract√®res, {page_count} pages")
                
                return {
                    "text": document.text,
                    "pages": page_count,
                    "chunk_file": str(pdf_path),
                    "char_count": char_count
                }
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Tentative {attempt + 1} √©chou√©e: {str(e)[:200]}")
                
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 3
                    logger.info(f"‚è≥ Attente {wait_time}s avant nouvelle tentative...")
                    time.sleep(wait_time)
                else:
                    # √âchec d√©finitif
                    error_msg = f"‚ùå √âCHEC D√âFINITIF extraction: {pdf_path.name} - {e}"
                    logger.error(error_msg)
                    raise RuntimeError(error_msg)
    
    def analyze_with_gemini_advanced(self, chunk: Dict, context: str = "", language: str = 'fr') -> Dict:
        """Analyse Gemini AVANC√âE avec mod√®le intelligent et double validation et support multilingue"""
        
        # Instructions de langue pour le prompt
        language_instruction = {
            'fr': "IMPORTANT: R√©ponds EXCLUSIVEMENT en fran√ßais avec tous les textes, descriptions et valeurs en fran√ßais.",
            'en': "IMPORTANT: Respond EXCLUSIVELY in English with all texts, descriptions and values in English."
        }
        
        lang_inst = language_instruction.get(language, language_instruction['fr'])
        
        # Prompt d√©taill√© pour extraction maximale d'informations
        detailed_prompt = f"""
Tu es un expert m√©dical sp√©cialis√© dans l'analyse d'instruments de laboratoire diagnostique.
Analyse EXHAUSTIVEMENT ce manuel m√©dical et extrait TOUTES les informations techniques et cliniques critiques.

{lang_inst}

CONTEXTE PR√âC√âDENT: {context[-300:] if context else "D√©but du document"}

SECTION √Ä ANALYSER: {chunk['description']} ({chunk['char_count']} caract√®res)

INSTRUCTIONS CRITIQUES:
- Extrait TOUS les d√©tails techniques, proc√©duraux et cliniques
- Identifie pr√©cis√©ment les volumes, concentrations, temp√©ratures, dur√©es
- Capture les sp√©cifications de performance, limites de d√©tection, gammes lin√©aires
- Rel√®ve TOUTES les pr√©cautions de s√©curit√©, contre-indications, limitations
- Documente les proc√©dures compl√®tes avec mat√©riels exacts
- Note les conditions de stockage, stabilit√©, contr√¥les qualit√©
- Extraie les donn√©es de validation clinique et performance analytique

TEXTE √Ä ANALYSER:
{chunk['text'][:80000]}

R√©ponds en JSON d√©taill√© PARFAITEMENT FORMAT√â:

{{
    "instrument": {{
        "nom": "nom exact complet de l'instrument",
        "fabricant": "fabricant exact",
        "modele": "mod√®le et r√©f√©rences exactes", 
        "type": "type d'instrument et technologie",
        "applications_cliniques": ["application clinique 1", "application clinique 2"],
        "principe_technique": "principe de fonctionnement d√©taill√©"
    }},
    "procedures": [
        {{
            "nom": "nom exact de l'analyse",
            "code_produit": "r√©f√©rence produit si mentionn√©e",
            "echantillon": {{
                "type": "type exact d'√©chantillon",
                "volume_minimum": "volume minimum requis",
                "volume_traitement": "volume de traitement",
                "anticoagulant": "anticoagulant requis",
                "conditions_prelevement": ["condition 1", "condition 2"]
            }},
            "preparation_echantillon": {{
                "etapes": ["√©tape d√©taill√©e 1", "√©tape d√©taill√©e 2"],
                "stabilite": "conditions et dur√©es de stabilit√©",
                "transport": "conditions de transport",
                "stockage": "conditions de stockage d√©taill√©es"
            }},
            "procedure_analytique": {{
                "etapes_detaillees": ["√©tape 1 avec d√©tails", "√©tape 2 avec d√©tails"],
                "duree_totale": "dur√©e compl√®te du processus",
                "temperature_incubation": "temp√©ratures si applicables",
                "cycles_pcr": "nombre de cycles si PCR",
                "detection": "m√©thode de d√©tection"
            }},
            "materiels_reactifs": {{
                "reactifs": ["r√©actif 1 avec r√©f√©rence", "r√©actif 2 avec r√©f√©rence"],
                "consommables": ["consommable 1", "consommable 2"],
                "equipements": ["√©quipement requis"]
            }},
            "performance": {{
                "gamme_lineaire": "gamme de mesure",
                "limite_detection": "limite de d√©tection",
                "limite_quantification": "limite de quantification",
                "precision": "donn√©es de pr√©cision",
                "reproductibilite": "donn√©es de reproductibilit√©"
            }},
            "controles_qualite": {{
                "controles_requis": ["contr√¥le positif", "contr√¥le n√©gatif"],
                "frequence": "fr√©quence des contr√¥les",
                "criteres_acceptation": ["crit√®re 1", "crit√®re 2"]
            }},
            "interpretation": {{
                "resultats_possibles": ["r√©sultat 1: signification", "r√©sultat 2: signification"],
                "seuils_decision": "seuils cliniques importants",
                "unites": "unit√©s de mesure",
                "facteur_conversion": "facteur de conversion si applicable"
            }},
            "precautions_critiques": [
                "pr√©caution de s√©curit√© 1 D√âTAILL√âE",
                "pr√©caution technique 2 D√âTAILL√âE"
            ],
            "limitations": [
                "limitation technique 1",
                "limitation clinique 2"
            ]
        }}
    ],
    "maintenance": [
        {{
            "type": "type exact de maintenance",
            "frequence": "fr√©quence pr√©cise",
            "duree": "temps n√©cessaire",
            "procedure_complete": {{
                "preparation": ["√©tape pr√©paration 1", "√©tape pr√©paration 2"],
                "execution": ["√©tape ex√©cution 1 d√©taill√©e", "√©tape ex√©cution 2 d√©taill√©e"],
                "verification": ["v√©rification 1", "v√©rification 2"],
                "documentation": "√©l√©ments √† documenter"
            }},
            "materiels_requis": ["mat√©riel 1", "mat√©riel 2"],
            "personnel": "qualification du personnel",
            "conditions_environnementales": "conditions requises"
        }}
    ],
    "specifications_techniques": [
        {{
            "categorie": "cat√©gorie technique pr√©cise",
            "parametres": [
                {{
                    "nom": "nom exact du param√®tre",
                    "valeur": "valeur exacte",
                    "unite": "unit√©",
                    "conditions": "conditions de mesure",
                    "tolerance": "tol√©rance acceptable"
                }}
            ]
        }}
    ],
    "securite": [
        {{
            "categorie": "cat√©gorie de risque",
            "risques_identifies": ["risque 1 d√©taill√©", "risque 2 d√©taill√©"],
            "mesures_prevention": ["mesure pr√©ventive 1", "mesure pr√©ventive 2"],
            "equipements_protection": ["EPI requis 1", "EPI requis 2"],
            "procedures_urgence": ["action urgence 1", "action urgence 2"],
            "formation_requise": "formation n√©cessaire",
            "reglementation": "r√©f√©rences r√©glementaires"
        }}
    ],
    "stockage_reagents": [
        {{
            "reagent": "nom du r√©actif",
            "temperature_stockage": "temp√©rature de stockage",
            "stabilite": "dur√©e de stabilit√©",
            "conditions_speciales": ["condition 1", "condition 2"],
            "duree_utilisation": "dur√©e apr√®s ouverture"
        }}
    ],
    "validation_clinique": {{
        "population_etudiee": "population des √©tudes cliniques",
        "nombre_echantillons": "nombre d'√©chantillons test√©s",
        "comparaison_methodes": "m√©thodes de r√©f√©rence",
        "sensibilite": "sensibilit√© analytique",
        "specificite": "sp√©cificit√© analytique",
        "etudes_interference": "substances test√©es pour interf√©rence",
        "genotypes_detectes": "g√©notypes ou variants d√©tect√©s"
    }},
    "calibration": [
        {{
            "type": "type de calibration",
            "frequence": "fr√©quence recommand√©e",
            "standards_utilises": ["standard 1", "standard 2"],
            "procedure": ["√©tape calibration 1", "√©tape calibration 2"],
            "criteres_acceptation": ["crit√®re 1", "crit√®re 2"],
            "tracabilite": "tra√ßabilit√© m√©trologique"
        }}
    ],
    "troubleshooting": [
        {{
            "probleme": "probl√®me identifi√© d√©taill√©",
            "causes_possibles": ["cause 1", "cause 2"],
            "solutions": ["solution 1 d√©taill√©e", "solution 2 d√©taill√©e"],
            "prevention": "mesures pr√©ventives"
        }}
    ],
    "resume_section": "r√©sum√© technique d√©taill√© de cette section en 3-4 phrases"
}}

CRITIQUE: Sois EXHAUSTIF, pr√©cis et technique. Capture TOUS les d√©tails num√©riques, proc√©duraux et cliniques. ASSURE-TOI que le JSON est PARFAITEMENT VALIDE."""
        
        # Utiliser Gemini 2.0 Flash si disponible
        try:
            # Essayer d'abord avec Gemini 2.0 Flash
            model_to_use = getattr(self, 'advanced_model', self.gemini_model)
            if hasattr(self, 'advanced_model') and self.advanced_model != self.gemini_model:
                model_name = "Gemini 2.0 Flash"
            else:
                model_name = self.config["gemini"]["model"]
        except:
            # Fallback vers le mod√®le configur√©
            model_to_use = self.gemini_model
            model_name = self.config["gemini"]["model"]
        
        for attempt in range(2):  # Maximum 2 tentatives
            try:
                logger.info(f"üß† Analyse {model_name} D√âTAILL√âE: {chunk['description']} (tentative {attempt + 1})")
                
                response = model_to_use.generate_content(detailed_prompt)
                
                if not response or not response.text:
                    raise ValueError("‚ùå Gemini n'a fourni aucune r√©ponse")
                
                response_text = response.text.strip()
                
                # DOUBLE VALIDATION avec second mod√®le
                validated_json = self.validate_and_fix_json_with_gemini(response_text)
                result = json.loads(validated_json)
                
                # Validation stricte de la structure d√©taill√©e
                self.validate_detailed_analysis_result(result)
                
                # Comptage d√©taill√© pour logs
                proc_count = len(result.get('procedures', []))
                maint_count = len(result.get('maintenance', []))
                spec_count = len(result.get('specifications_techniques', []))
                sec_count = len(result.get('securite', []))
                storage_count = len(result.get('stockage_reagents', []))
                
                logger.info(f"‚úÖ Analyse D√âTAILL√âE r√©ussie avec {model_name}: {proc_count} proc√©dures, {maint_count} maintenances, {spec_count} sp√©cs, {sec_count} s√©curit√©s, {storage_count} stockages")
                
                return result
                
            except json.JSONDecodeError as e:
                logger.error(f"‚ùå JSON invalide m√™me apr√®s double validation (tentative {attempt + 1}): {e}")
                if attempt < 1:
                    time.sleep(2)
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur analyse Gemini avanc√©e (tentative {attempt + 1}): {e}")
                if attempt < 1:
                    time.sleep(3)
        
        # √âchec d√©finitif = erreur critique
        error_msg = f"‚ùå √âCHEC ANALYSE GEMINI AVANC√âE: {chunk['description']}"
        logger.error(error_msg)
        raise RuntimeError(error_msg)
    
    def validate_and_fix_json_with_gemini(self, response_text: str) -> str:
        """Double validation: utilise Gemini pour corriger le JSON d√©faillant"""
        
        # D'abord, essayer l'extraction normale
        try:
            return self.extract_and_validate_json(response_text)
        except (ValueError, json.JSONDecodeError) as e:
            logger.warning(f"‚ö†Ô∏è JSON invalide d√©tect√©, tentative de correction automatique: {e}")
            
            # Utiliser Gemini pour corriger le JSON
            validation_prompt = f"""
Tu es un expert en formatage JSON. Le JSON suivant contient des erreurs de syntaxe. 
Corrige-le pour qu'il soit parfaitement valide tout en pr√©servant TOUTES les informations.

JSON √Ä CORRIGER:
{response_text[:10000]}

INSTRUCTIONS:
1. Garde TOUTES les informations existantes
2. Corrige uniquement les erreurs de syntaxe JSON
3. Assure-toi que tous les guillemets sont corrects
4. Supprime les virgules en trop
5. V√©rifie que tous les crochets et accolades sont √©quilibr√©s
6. √âchappe correctement les caract√®res sp√©ciaux dans les strings

R√©ponds UNIQUEMENT avec le JSON corrig√©, sans explanation, sans markdown."""
            
            try:
                # Utiliser un mod√®le plus simple pour la correction
                correction_response = self.gemini_model.generate_content(validation_prompt)
                
                if correction_response and correction_response.text:
                    corrected_text = correction_response.text.strip()
                    
                    # Extraire et valider le JSON corrig√©
                    return self.extract_and_validate_json(corrected_text)
                else:
                    raise ValueError("‚ùå Gemini n'a pas pu corriger le JSON")
                    
            except Exception as correction_error:
                logger.error(f"‚ùå √âchec correction JSON avec Gemini: {correction_error}")
                # Fallback vers correction automatique basique
                return self.extract_and_validate_json(response_text)
    
    def extract_and_validate_json(self, response_text: str) -> str:
        """Extraction et validation JSON avec correction automatique"""
        if not response_text or not response_text.strip():
            raise ValueError("‚ùå R√©ponse Gemini vide")
        
        # Supprimer les blocs markdown
        text = response_text
        if "```json" in text:
            start = text.find("```json") + 7
            end = text.rfind("```")
            if end > start:
                text = text[start:end]
        elif "```" in text:
            start = text.find("```") + 3
            end = text.rfind("```")
            if end > start:
                text = text[start:end]
        
        # Extraire JSON entre accolades
        first_brace = text.find("{")
        last_brace = text.rfind("}")
        
        if first_brace < 0 or last_brace < 0 or first_brace >= last_brace:
            raise ValueError("‚ùå Aucun JSON valide trouv√© dans la r√©ponse Gemini")
        
        json_text = text[first_brace:last_brace + 1].strip()
        
        # Corrections automatiques des erreurs JSON courantes
        json_text = self.fix_common_json_errors(json_text)
        
        # Validation JSON basique
        try:
            json.loads(json_text)  # Test de parsing
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå JSON encore invalide apr√®s corrections: {e}")
            logger.error(f"Extrait probl√©matique: {json_text[max(0, e.pos-50):e.pos+50]}")
            raise ValueError(f"‚ùå JSON invalide m√™me apr√®s corrections: {e}")
        
        return json_text
    
    def fix_common_json_errors(self, json_text: str) -> str:
        """Corrige automatiquement les erreurs JSON courantes"""
        
        # 1. Supprimer les virgules en fin d'objet/array
        json_text = re.sub(r',(\s*[}\]])', r'\1', json_text)
        
        # 2. √âchapper les guillemets dans les valeurs
        # Prot√©ger d'abord les structures JSON valides
        lines = json_text.split('\n')
        corrected_lines = []
        
        for line in lines:
            # Si la ligne contient une valeur de string
            if '": "' in line and not line.strip().endswith(',') and not line.strip().endswith('{') and not line.strip().endswith('['):
                # Trouver la partie valeur apr√®s ": "
                parts = line.split('": "', 1)
                if len(parts) == 2:
                    key_part = parts[0]
                    value_part = parts[1]
                    
                    # Trouver la fin de la valeur
                    end_quote = value_part.rfind('"')
                    if end_quote > 0:
                        value_content = value_part[:end_quote]
                        rest = value_part[end_quote:]
                        
                        # √âchapper les guillemets dans la valeur seulement
                        value_content = value_content.replace('"', '\\"')
                        
                        line = key_part + '": "' + value_content + rest
            
            corrected_lines.append(line)
        
        json_text = '\n'.join(corrected_lines)
        
        # 3. Supprimer les caract√®res de contr√¥le probl√©matiques
        json_text = re.sub(r'[\x00-\x1f\x7f-\x9f]', ' ', json_text)
        
        # 4. R√©parer les arrays mal ferm√©s
        json_text = re.sub(r',(\s*\])', r'\1', json_text)
        
        # 5. Supprimer les doubles virgules
        json_text = re.sub(r',,+', ',', json_text)
        
        return json_text
    
    def validate_detailed_analysis_result(self, result: Dict):
        """Validation simplifi√©e du r√©sultat d'analyse"""
        if not isinstance(result, dict):
            raise ValueError("‚ùå R√©sultat d'analyse n'est pas un dictionnaire")
        
        # Structure simplifi√©e requise
        required_sections = [
            'instrument', 'procedures', 'maintenance', 'specifications_techniques', 
            'securite', 'stockage_reagents', 'validation_clinique', 'calibration', 
            'troubleshooting', 'resume_section'
        ]
        
        for section in required_sections:
            if section not in result:
                if section in ['instrument', 'validation_clinique', 'resume_section']:
                    result[section] = {} if section != 'resume_section' else ""
                else:
                    result[section] = []
        
        # Validation des proc√©dures (critique pour s√©curit√©)
        procedures = result.get('procedures', [])
        if not isinstance(procedures, list):
            result['procedures'] = []
        
        # Validation maintenance (critique pour fiabilit√©)
        maintenance = result.get('maintenance', [])
        if not isinstance(maintenance, list):
            result['maintenance'] = []
        
        # S'assurer que l'instrument a au moins un nom
        if not result.get('instrument', {}).get('nom'):
            if result.get('instrument'):
                result['instrument']['nom'] = "Instrument non identifi√©"
            else:
                result['instrument'] = {"nom": "Instrument non identifi√©"}
        
        logger.debug("‚úÖ Structure d'analyse simplifi√©e valid√©e")
        
        # Log de debug pour voir ce qui a √©t√© extrait
        proc_count = len(result.get('procedures', []))
        maint_count = len(result.get('maintenance', []))
        logger.info(f"‚úÖ Validation OK: {proc_count} proc√©dures, {maint_count} maintenances")
        
        return result
    
    def synthesize_final_medical(self, analyses: List[Dict], language: str = 'fr') -> Dict:
        """Synth√®se finale EXHAUSTIVE avec double validation JSON"""
        if not analyses:
            raise ValueError("‚ùå Aucune analyse √† synth√©tiser")
        
        logger.info("üî¨ Cr√©ation de la synth√®se m√©dicale D√âTAILL√âE...")
        
        # Compilation EXHAUSTIVE des donn√©es
        all_procedures = []
        all_maintenance = []
        all_specs = []
        all_security = []
        all_calibration = []
        all_storage = []
        all_validation = []
        all_troubleshooting = []
        instrument_info = {}
        
        for i, analysis in enumerate(analyses):
            if not isinstance(analysis, dict):
                logger.warning(f"‚ö†Ô∏è Analyse {i+1} invalide - ignor√©e")
                continue
            
            # Consolidation d√©taill√©e
            all_procedures.extend(analysis.get("procedures", []))
            all_maintenance.extend(analysis.get("maintenance", []))
            all_specs.extend(analysis.get("specifications_techniques", []))
            all_security.extend(analysis.get("securite", []))
            all_calibration.extend(analysis.get("calibration", []))
            all_storage.extend(analysis.get("stockage_reagents", []))
            all_troubleshooting.extend(analysis.get("troubleshooting", []))
            
            # Validation clinique (important)
            validation = analysis.get("validation_clinique", {})
            if validation and isinstance(validation, dict):
                all_validation.append(validation)
            
            # Consolidation info instrument
            inst = analysis.get("instrument", {})
            if isinstance(inst, dict):
                for key, value in inst.items():
                    if value and str(value).strip() and not str(value).startswith("Non"):
                        if key not in instrument_info or len(str(value)) > len(str(instrument_info.get(key, ""))):
                            instrument_info[key] = value
        
        logger.info(f"üìä Donn√©es D√âTAILL√âES compil√©es:")
        logger.info(f"   - Proc√©dures: {len(all_procedures)}")
        logger.info(f"   - Maintenances: {len(all_maintenance)}")
        logger.info(f"   - Sp√©cifications: {len(all_specs)}")
        logger.info(f"   - S√©curit√©: {len(all_security)}")
        logger.info(f"   - Stockage: {len(all_storage)}")
        logger.info(f"   - Validation: {len(all_validation)}")
        logger.info(f"   - Troubleshooting: {len(all_troubleshooting)}")
        
        # Synth√®se finale EXHAUSTIVE avec Gemini + double validation
        # Instructions de langue pour le prompt
        logger.info(f"DEBUG: About to use language variable: {language}")
        language_instruction = {
            'fr': "IMPORTANT: R√©ponds EXCLUSIVEMENT en fran√ßais avec tous les textes, descriptions et valeurs en fran√ßais.",
            'en': "IMPORTANT: Respond EXCLUSIVELY in English with all texts, descriptions and values in English."
        }
        lang_inst = language_instruction.get(language, language_instruction['fr'])
        logger.info(f"DEBUG: Language instruction set to: {lang_inst[:50]}...")
        synthesis_prompt = f"""
Tu es un expert m√©dical diagnostique. Cr√©er une synth√®se TECHNIQUE COMPL√àTE de cet instrument m√©dical.

{lang_inst}

INFORMATIONS INSTRUMENT CONSOLID√âES:
{json.dumps(instrument_info, ensure_ascii=False, indent=2)}

DONN√âES EXHAUSTIVES ANALYS√âES:

PROC√âDURES D√âTAILL√âES ({len(all_procedures)} extraites):
{json.dumps(all_procedures[:3], ensure_ascii=False, indent=2) if all_procedures else "Aucune proc√©dure extraite"}

MAINTENANCE PR√âVENTIVE ({len(all_maintenance)} extraites):
{json.dumps(all_maintenance[:3], ensure_ascii=False, indent=2) if all_maintenance else "Aucune maintenance extraite"}

SP√âCIFICATIONS TECHNIQUES ({len(all_specs)} extraites):
{json.dumps(all_specs[:2], ensure_ascii=False, indent=2) if all_specs else "Aucune sp√©cification extraite"}

S√âCURIT√â ET PR√âCAUTIONS ({len(all_security)} extraites):
{json.dumps(all_security[:2], ensure_ascii=False, indent=2) if all_security else "Aucune pr√©caution extraite"}

STOCKAGE R√âACTIFS ({len(all_storage)} extraits):
{json.dumps(all_storage[:2], ensure_ascii=False, indent=2) if all_storage else "Aucun stockage extrait"}

VALIDATION CLINIQUE:
{json.dumps(all_validation[:2], ensure_ascii=False, indent=2) if all_validation else "Aucune validation extraite"}

D√âPANNAGE ({len(all_troubleshooting)} extraits):
{json.dumps(all_troubleshooting[:2], ensure_ascii=False, indent=2) if all_troubleshooting else "Aucun d√©pannage extrait"}

MISSION CRITIQUE: Consolider TOUTES ces donn√©es en une synth√®se technique COMPL√àTE pour usage m√©dical professionnel. Pr√©server tous les d√©tails techniques critiques (volumes, concentrations, r√©f√©rences, limites, performances).

JSON SYNTH√àSE M√âDICALE EXHAUSTIVE PARFAITEMENT FORMAT√â:

{{
    "informations_generales": {{
        "nom_instrument": "nom complet consolid√© avec toutes r√©f√©rences",
        "fabricant": "fabricant exact",
        "modele": "mod√®le complet avec r√©f√©rences produit",
        "type_instrument": "type d'instrument et technologie pr√©cise",
        "applications_principales": [
            "application clinique d√©taill√©e 1 avec contexte",
            "application clinique d√©taill√©e 2 avec contexte"
        ],
        "principe_fonctionnement": "principe technique d√©taill√© de fonctionnement",
        "approche_diagnostique": "m√©thodologie diagnostique et workflow"
    }},
    "procedures_analyses": [
        {{
            "nom_analyse": "nom complet de l'analyse avec code produit",
            "indication_clinique": "indication m√©dicale pr√©cise et population cible",
            "echantillon": {{
                "type": "type d'√©chantillon exact avec sp√©cifications",
                "volume_minimum": "volume minimum avec justification",
                "volume_traitement": "volume de traitement avec options",
                "anticoagulant": "anticoagulant sp√©cifique avec alternatives"
            }},
            "preparation_detaillee": {{
                "etapes": [
                    "√©tape pr√©paration 1 avec volumes/temps pr√©cis",
                    "√©tape pr√©paration 2 avec conditions/temp√©ratures"
                ],
                "stabilite": "conditions de stabilit√© compl√®tes avec dur√©es",
                "stockage": "conditions de stockage d√©taill√©es par phase"
            }},
            "procedure_analytique": {{
                "workflow": [
                    "√©tape analytique 1 avec param√®tres techniques",
                    "√©tape analytique 2 avec conditions de traitement"
                ],
                "duree_totale": "temps total avec d√©composition par phase",
                "conditions_techniques": "temp√©ratures, pressions, vitesses d√©taill√©es"
            }},
            "performance_analytique": {{
                "gamme_mesure": "gamme lin√©aire compl√®te avec unit√©s",
                "limite_detection": "LoD pr√©cise avec conditions de validation",
                "precision": "donn√©es de pr√©cision intra et inter-s√©rie (CV%)"
            }},
            "controles_qualite": {{
                "types_controles": [
                    "contr√¥le positif haut avec concentration cible",
                    "contr√¥le n√©gatif avec crit√®res acceptation"
                ],
                "frequence": "fr√©quence des contr√¥les avec justification"
            }},
            "precautions_critiques": [
                "S√âCURIT√â BIOLOGIQUE: manipulation √©chantillons infectieux avec EPI",
                "QUALIT√â ANALYTIQUE: pr√©vention contamination crois√©e"
            ]
        }}
    ],
    "maintenance_preventive": [
        {{
            "type_maintenance": "maintenance d√©taill√©e avec niveau d'intervention",
            "frequence_precise": "fr√©quence exacte avec conditions d√©clenchantes",
            "duree_estimee": "temps n√©cessaire avec marge",
            "procedure_step_by_step": {{
                "preparation": [
                    "pr√©paration 1: mat√©riels avec r√©f√©rences"
                ],
                "execution": [
                    "√©tape 1: proc√©dure avec param√®tres techniques"
                ],
                "verification": [
                    "contr√¥le 1: param√®tre avec limite acceptable"
                ]
            }},
            "materiels_specifiques": [
                "mat√©riel 1 avec r√©f√©rence et sp√©cifications"
            ]
        }}
    ],
    "guide_utilisation_quotidienne": {{
        "demarrage_systeme": [
            "startup 1: v√©rifications pr√©alables avec check-list",
            "startup 2: initialisation avec param√®tres de contr√¥le"
        ],
        "arret_systeme": [
            "shutdown 1: finalisation analyses en cours avec sauvegarde",
            "shutdown 2: mise en s√©curit√© avec v√©rifications"
        ],
        "maintenance_quotidienne": [
            "t√¢che quotidienne 1: contr√¥les de routine avec documentation"
        ]
    }},
    "resume_executif": "R√©sum√© technique et clinique EXHAUSTIF de l'instrument couvrant: technologie utilis√©e, applications cliniques principales, performances analytiques cl√©s, exigences d'utilisation, consid√©rations de maintenance et points critiques de s√©curit√© pour usage m√©dical professionnel"
}}

EXIGENCE ABSOLUE: JSON PARFAITEMENT VALIDE. Consolide EXHAUSTIVEMENT toutes les donn√©es en pr√©servant les d√©tails techniques critiques."""
        
        try:
            # Utiliser le mod√®le avanc√© si disponible
            model_to_use = getattr(self, 'advanced_model', self.gemini_model)
            
            response = model_to_use.generate_content(synthesis_prompt)
            
            if not response or not response.text:
                raise ValueError("‚ùå Gemini n'a pas g√©n√©r√© de synth√®se")
            
            # Double validation avec correction automatique
            cleaned_json = self.validate_and_fix_json_with_gemini(response.text.strip())
            result = json.loads(cleaned_json)
            
            # Validation de la synth√®se finale d√©taill√©e
            self.validate_comprehensive_synthesis(result)
            
            logger.info("‚úÖ Synth√®se m√©dicale EXHAUSTIVE cr√©√©e et valid√©e avec double validation")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå √âCHEC SYNTH√àSE D√âTAILL√âE: {e}")
            # Fallback: cr√©er une synth√®se de secours
            logger.warning("üö® Cr√©ation synth√®se de secours d√©taill√©e...")
            return self.create_comprehensive_fallback_synthesis(instrument_info, all_procedures, all_maintenance, all_specs, all_security)
    
    def validate_comprehensive_synthesis(self, synthesis: Dict):
        """Validation stricte de la synth√®se exhaustive"""
        if not isinstance(synthesis, dict):
            raise ValueError("‚ùå Synth√®se exhaustive invalide")
        
        critical_sections = [
            'informations_generales', 'procedures_analyses', 'maintenance_preventive'
        ]
        
        missing_critical = [sec for sec in critical_sections if not synthesis.get(sec)]
        
        if missing_critical:
            logger.warning(f"‚ö†Ô∏è ATTENTION: Sections critiques manquantes: {missing_critical}")
        
        # V√©rifier r√©sum√© ex√©cutif
        resume = synthesis.get('resume_executif', '')
        if not resume or len(resume.strip()) < 100:
            logger.warning("‚ö†Ô∏è R√©sum√© ex√©cutif insuffisant ou manquant")
        
        # V√©rifier proc√©dures (critique pour s√©curit√©)
        procedures = synthesis.get('procedures_analyses', [])
        if not procedures:
            logger.warning("‚ö†Ô∏è ATTENTION CRITIQUE: Aucune proc√©dure d'analyse identifi√©e")
        
        # V√©rifier maintenance (critique pour fiabilit√©)
        maintenance = synthesis.get('maintenance_preventive', [])
        if not maintenance:
            logger.warning("‚ö†Ô∏è ATTENTION: Aucune proc√©dure de maintenance identifi√©e")
        
        logger.info("‚úÖ Synth√®se exhaustive m√©dicale valid√©e")
    
    def create_comprehensive_fallback_synthesis(self, instrument_info: Dict, procedures: List, maintenance: List, specs: List, security: List) -> Dict:
        """Cr√©e une synth√®se de secours exhaustive en cas d'√©chec Gemini"""
        logger.warning("üö® Cr√©ation synth√®se de secours EXHAUSTIVE")
        
        return {
            "informations_generales": {
                "nom_instrument": instrument_info.get('nom', 'Instrument non identifi√©'),
                "fabricant": instrument_info.get('fabricant', 'Non sp√©cifi√©'),
                "modele": instrument_info.get('modele', 'Non sp√©cifi√©'),
                "type_instrument": instrument_info.get('type', 'Analyseur m√©dical'),
                "applications_principales": instrument_info.get('applications_cliniques', [])[:5],
                "principe_fonctionnement": instrument_info.get('principe_technique', 'Non sp√©cifi√©'),
                "approche_diagnostique": "M√©thodologie diagnostique selon manuel"
            },
            "procedures_analyses": [
                {
                    "nom_analyse": proc.get('nom', 'Analyse non sp√©cifi√©e'),
                    "indication_clinique": "Indication selon manuel complet",
                    "echantillon": {
                        "type": proc.get('echantillon', {}).get('type', 'Non sp√©cifi√©'),
                        "volume_minimum": proc.get('echantillon', {}).get('volume_minimum', 'Voir manuel'),
                        "volume_traitement": proc.get('echantillon', {}).get('volume_traitement', 'Voir manuel'),
                        "anticoagulant": proc.get('echantillon', {}).get('anticoagulant', 'Selon proc√©dure')
                    },
                    "preparation_detaillee": {
                        "etapes": proc.get('preparation_echantillon', {}).get('etapes', [])[:5],
                        "stabilite": proc.get('preparation_echantillon', {}).get('stabilite', 'Voir manuel'),
                        "stockage": proc.get('preparation_echantillon', {}).get('stockage', 'Conditions standard')
                    },
                    "procedure_analytique": {
                        "workflow": proc.get('procedure_analytique', {}).get('etapes_detaillees', [])[:6],
                        "duree_totale": proc.get('procedure_analytique', {}).get('duree_totale', 'Voir manuel'),
                        "conditions_techniques": proc.get('procedure_analytique', {}).get('temperature_incubation', 'Conditions contr√¥l√©es')
                    },
                    "performance_analytique": {
                        "gamme_mesure": proc.get('performance', {}).get('gamme_lineaire', 'Voir sp√©cifications'),
                        "limite_detection": proc.get('performance', {}).get('limite_detection', 'Selon validation'),
                        "precision": proc.get('performance', {}).get('precision', 'Donn√©es de validation')
                    },
                    "controles_qualite": {
                        "types_controles": proc.get('controles_qualite', {}).get('controles_requis', [])[:3],
                        "frequence": proc.get('controles_qualite', {}).get('frequence', 'Selon proc√©dure')
                    },
                    "precautions_critiques": proc.get('precautions_critiques', [])[:4]
                }
                for proc in procedures[:5]  # Max 5 proc√©dures
            ],
            "maintenance_preventive": [
                {
                    "type_maintenance": maint.get('type', 'Maintenance non sp√©cifi√©e'),
                    "frequence_precise": maint.get('frequence', 'Non sp√©cifi√©e'),
                    "duree_estimee": maint.get('duree', 'Selon complexit√©'),
                    "procedure_step_by_step": {
                        "preparation": maint.get('procedure_complete', {}).get('preparation', [])[:3],
                        "execution": maint.get('procedure_complete', {}).get('execution', [])[:4],
                        "verification": maint.get('procedure_complete', {}).get('verification', [])[:3]
                    },
                    "materiels_specifiques": maint.get('materiels_requis', [])[:3]
                }
                for maint in maintenance[:6]  # Max 6 maintenances
            ],
            "guide_utilisation_quotidienne": {
                "demarrage_systeme": [
                    "V√©rifier l'alimentation √©lectrique",
                    "Contr√¥ler les niveaux de r√©actifs",
                    "Effectuer les contr√¥les qualit√©",
                    "Valider le fonctionnement syst√®me"
                ],
                "arret_systeme": [
                    "Finaliser toutes les analyses en cours",
                    "Effectuer le nettoyage syst√®me",
                    "Sauvegarder les donn√©es",
                    "Mise en s√©curit√©"
                ],
                "maintenance_quotidienne": [
                    "Contr√¥les visuels de l'instrument",
                    "Nettoyage des surfaces externes",
                    "V√©rification des niveaux",
                    "Documentation des observations"
                ]
            },
            "resume_executif": f"Synth√®se technique exhaustive de l'instrument {instrument_info.get('nom', 'non identifi√©')} g√©n√©r√©e automatiquement. {len(procedures)} proc√©dures d'analyse, {len(maintenance)} op√©rations de maintenance identifi√©es. Cette synth√®se consolid√©e couvre les aspects critiques d'utilisation, de maintenance et de s√©curit√© pour usage m√©dical professionnel. V√©rification obligatoire avec le manuel complet avant mise en service clinique."
        }
    
    def merge_texts_medical(self, chunk_results: List[Dict]) -> str:
        """Fusion stricte des textes avec validation m√©dicale"""
        full_text = ""
        successful_chunks = 0
        total_chars = 0
        
        for i, chunk_result in enumerate(chunk_results):
            if chunk_result.get("text") and len(chunk_result["text"]) > 50:  # Validation stricte
                chunk_text = chunk_result["text"]
                char_count = len(chunk_text)
                
                # En-t√™te de section m√©dicale
                section_header = f"\n\n{'='*80}\nSECTION M√âDICALE {i+1}\nPages: {chunk_result.get('pages', '?')}\nCaract√®res: {char_count:,}\nSource: {Path(chunk_result.get('chunk_file', '')).name}\n{'='*80}\n\n"
                
                full_text += section_header + chunk_text
                successful_chunks += 1
                total_chars += char_count
                
                logger.info(f"‚úÖ Section {i+1} int√©gr√©e: {char_count:,} caract√®res")
            else:
                logger.error(f"‚ùå Section {i+1} invalide - texte insuffisant")
                raise ValueError(f"Section {i+1} contient un texte insuffisant pour analyse m√©dicale")
        
        if successful_chunks == 0:
            raise ValueError("‚ùå Aucune section valide pour analyse m√©dicale")
        
        logger.info(f"‚úÖ Fusion m√©dicale: {successful_chunks} sections, {total_chars:,} caract√®res")
        return full_text
    
    def create_analysis_chunks(self, text: str) -> List[Dict]:
        """Cr√©ation de chunks d'analyse optimis√©s pour Gemini"""
        if not text or len(text.strip()) < 100:
            raise ValueError("‚ùå Texte insuffisant pour cr√©ation de chunks")
        
        # Limite Gemini conservative
        max_chars_per_chunk = 350000
        
        chunks = []
        
        if len(text) <= max_chars_per_chunk:
            chunks.append({
                "text": text,
                "chunk_id": 0,
                "description": "Document m√©dical complet",
                "char_count": len(text)
            })
            logger.info(f"üìÑ Document analysable en 1 chunk: {len(text):,} caract√®res")
            return chunks
        
        # D√©coupage par sections marqu√©es
        section_markers = re.findall(r'={80}\nSECTION M√âDICALE \d+', text)
        logger.info(f"üìä {len(section_markers)} sections m√©dicales identifi√©es")
        
        if len(section_markers) > 1:
            # D√©coupage intelligent par sections
            sections = re.split(r'={80}\nSECTION M√âDICALE \d+[^\n]*\n={80}\n\n', text)
            
            current_chunk = ""
            chunk_sections = []
            
            for i, section in enumerate(sections):
                if not section.strip():
                    continue
                
                if len(current_chunk) + len(section) > max_chars_per_chunk and current_chunk:
                    chunks.append({
                        "text": current_chunk,
                        "chunk_id": len(chunks),
                        "description": f"Sections m√©dicales {chunk_sections[0]}-{chunk_sections[-1]}" if len(chunk_sections) > 1 else f"Section m√©dicale {chunk_sections[0]}",
                        "char_count": len(current_chunk)
                    })
                    
                    current_chunk = ""
                    chunk_sections = []
                
                current_chunk += section
                chunk_sections.append(i+1)
            
            # Chunk final
            if current_chunk.strip():
                chunks.append({
                    "text": current_chunk,
                    "chunk_id": len(chunks),
                    "description": f"Sections finales {chunk_sections[0]}-{chunk_sections[-1]}" if len(chunk_sections) > 1 else f"Section finale {chunk_sections[0]}",
                    "char_count": len(current_chunk)
                })
        
        else:
            # D√©coupage simple par taille
            for i in range(0, len(text), max_chars_per_chunk):
                chunk_text = text[i:i + max_chars_per_chunk]
                
                if chunk_text.strip():
                    chunks.append({
                        "text": chunk_text,
                        "chunk_id": len(chunks),
                        "description": f"Partie m√©dicale {len(chunks) + 1}",
                        "char_count": len(chunk_text)
                    })
        
        if not chunks:
            raise ValueError("‚ùå √âchec cr√©ation des chunks d'analyse")
        
        logger.info(f"‚úÖ {len(chunks)} chunks d'analyse cr√©√©s:")
        for chunk in chunks:
            logger.info(f"   - {chunk['description']}: {chunk['char_count']:,} caract√®res")
        
        return chunks
    
    def analyze_manual_organized(self, pdf_path: Path, language: str = 'fr') -> Dict:
        """Analyse compl√®te stricte d'un manuel m√©dical avec extraction exhaustive et support multilingue"""
        logger.info(f"üè• D√âBUT ANALYSE M√âDICALE EXHAUSTIVE: {pdf_path.name} (Language: {language})")
        
        pdf_chunks = []
        prepared_pdf = None
        
        try:
            # 1. Pr√©paration et validation PDF
            prepared_pdf = self.prepare_pdf(pdf_path)
            
            # 2. D√©coupage strict en chunks ‚â§15 pages
            pdf_chunks = self.split_pdf_15pages(prepared_pdf)
            logger.info(f"üìÑ {len(pdf_chunks)} chunks PDF cr√©√©s")
            
            # 3. Extraction de texte stricte
            chunk_texts = []
            
            for i, chunk_pdf in enumerate(pdf_chunks):
                logger.info(f"üîç Extraction chunk {i+1}/{len(pdf_chunks)}")
                
                result = self.extract_text_safe(chunk_pdf)  # L√®ve exception si √©chec
                chunk_texts.append(result)
                
                # D√©lai entre extractions (respect des quotas)
                if i < len(pdf_chunks) - 1:
                    delay = self.config["analysis"]["delay_between_requests"]
                    time.sleep(delay)
            
            # 4. Fusion et structuration du texte
            full_text = self.merge_texts_medical(chunk_texts)
            
            if not full_text or len(full_text.strip()) < 500:
                raise ValueError("‚ùå Texte extrait insuffisant pour analyse m√©dicale")
            
            # 5. Cr√©ation des chunks d'analyse Gemini
            analysis_chunks = self.create_analysis_chunks(full_text)
            
            if not analysis_chunks:
                raise ValueError("‚ùå Impossible de cr√©er des chunks d'analyse")
            
            # 6. Analyse Gemini EXHAUSTIVE de chaque chunk
            chunk_analyses = []
            context = ""
            
            for i, chunk in enumerate(analysis_chunks):
                logger.info(f"üß† Analyse Gemini EXHAUSTIVE {i+1}/{len(analysis_chunks)}: {chunk['description']}")
                
                analysis = self.analyze_with_gemini_advanced(chunk, context, language)  # Version avanc√©e avec double validation
                chunk_analyses.append(analysis)
                
                # Contexte enrichi pour chunk suivant
                context += f"{chunk['description']}: {analysis.get('resume_section', '')}\n"
                if analysis.get('instrument', {}).get('nom'):
                    context += f"Instrument: {analysis['instrument']['nom']}\n"
                
                # D√©lai entre analyses Gemini
                if i < len(analysis_chunks) - 1:
                    delay = self.config["analysis"]["delay_between_requests"]
                    time.sleep(delay)
            
            # 7. Synth√®se finale m√©dicale EXHAUSTIVE
            logger.info("üî¨ Cr√©ation de la synth√®se m√©dicale EXHAUSTIVE...")
            synthesis = self.synthesize_final_medical(chunk_analyses, language)
            
            # 8. G√©n√©ration du PDF LaTeX PROFESSIONNEL avec donn√©es compl√®tes
            output_pdf = self.syntheses_dir / f"{pdf_path.stem}_SYNTHESE_MEDICALE_COMPLETE.pdf"
            
            logger.info("üìÑ G√©n√©ration du PDF m√©dical professionnel...")
            pdf_success = self.latex_generator.generate_latex_synthesis(
                synthesis, output_pdf, f"MANUEL M√âDICAL COMPLET - {pdf_path.stem}", language
            )
            
            if not pdf_success:
                raise RuntimeError("‚ùå √âCHEC G√âN√âRATION PDF M√âDICAL - Aucun fichier cr√©√©")
            
            # 9. Nettoyage strict des fichiers temporaires
            self.cleanup_temp_files(pdf_chunks, prepared_pdf, pdf_path)
            
            # 10. Statistiques exhaustives
            stats = self.calculate_comprehensive_stats(pdf_chunks, analysis_chunks, chunk_texts, chunk_analyses, synthesis)
            
            logger.info(f"‚úÖ ANALYSE M√âDICALE EXHAUSTIVE TERMIN√âE: {pdf_path.name}")
            logger.info(f"üìÑ PDF m√©dical complet cr√©√©: {output_pdf}")
            
            return {
                "success": True,
                "pdf_medical": str(output_pdf),
                "statistiques": stats,
                "synthesis": synthesis,
                "validation": "CONFORME USAGE M√âDICAL PROFESSIONNEL"
            }
            
        except Exception as e:
            # Nettoyage en cas d'erreur
            self.emergency_cleanup(pdf_chunks, prepared_pdf, pdf_path)
            
            error_msg = f"‚ùå √âCHEC ANALYSE M√âDICALE EXHAUSTIVE: {pdf_path.name} - {e}"
            logger.error(error_msg)
            
            return {
                "success": False,
                "error": str(e),
                "pdf_path": str(pdf_path),
                "validation": "√âCHEC - NON CONFORME"
            }
    
    def calculate_comprehensive_stats(self, pdf_chunks: List[Path], analysis_chunks: List[Dict], 
                                    chunk_texts: List[Dict], chunk_analyses: List[Dict], synthesis: Dict) -> Dict:
        """Calcul des statistiques exhaustives"""
        
        # Compter les √©l√©ments dans la synth√®se finale
        procedures = synthesis.get("procedures_analyses", [])
        maintenance = synthesis.get("maintenance_preventive", [])
        
        # Statistiques d√©taill√©es par proc√©dure
        procedures_with_performance = sum(1 for p in procedures if p.get("performance_analytique"))
        procedures_with_controls = sum(1 for p in procedures if p.get("controles_qualite"))
        procedures_with_precautions = sum(1 for p in procedures if p.get("precautions_critiques"))
        
        # Statistiques de maintenance
        maintenance_with_timing = sum(1 for m in maintenance if m.get("duree_estimee"))
        maintenance_with_materials = sum(1 for m in maintenance if m.get("materiels_specifiques"))
        
        return {
            "extraction": {
                "chunks_pdf_trait√©s": len(pdf_chunks),
                "chunks_analys√©s": len(analysis_chunks),
                "caract√®res_totaux": sum(result.get("char_count", 0) for result in chunk_texts),
                "extractions_r√©ussies": len([r for r in chunk_texts if r.get("text") and len(r["text"]) > 50])
            },
            "analyse_brute": {
                "proc√©dures_brutes": sum(len(a.get("procedures", [])) for a in chunk_analyses),
                "maintenances_brutes": sum(len(a.get("maintenance", [])) for a in chunk_analyses),
                "sp√©cifications_brutes": sum(len(a.get("specifications_techniques", [])) for a in chunk_analyses),
                "√©l√©ments_s√©curit√©": sum(len(a.get("securite", [])) for a in chunk_analyses),
                "√©l√©ments_stockage": sum(len(a.get("stockage_reagents", [])) for a in chunk_analyses)
            },
            "synthese_finale": {
                "proc√©dures_consolid√©es": len(procedures),
                "proc√©dures_avec_performance": procedures_with_performance,
                "proc√©dures_avec_contr√¥les": procedures_with_controls,
                "proc√©dures_avec_pr√©cautions": procedures_with_precautions,
                "maintenances_consolid√©es": len(maintenance),
                "maintenances_avec_timing": maintenance_with_timing,
                "maintenances_avec_mat√©riels": maintenance_with_materials
            },
            "qualit√©_extraction": {
                "instrument_identifi√©": bool(synthesis.get("informations_generales", {}).get("nom_instrument")),
                "principe_technique_document√©": bool(synthesis.get("informations_generales", {}).get("principe_fonctionnement")),
                "guide_utilisation_complet": bool(synthesis.get("guide_utilisation_quotidienne")),
                "r√©sum√©_ex√©cutif_longueur": len(synthesis.get("resume_executif", "")),
                "niveau_d√©tail": "EXHAUSTIF" if len(procedures) > 0 and procedures_with_performance > 0 else "BASIQUE"
            },
            "conformit√©_m√©dicale": {
                "donn√©es_performance_analytique": procedures_with_performance > 0,
                "pr√©cautions_s√©curit√©_document√©es": procedures_with_precautions > 0,
                "maintenance_pr√©ventive_structur√©e": len(maintenance) > 0,
                "guide_utilisation_quotidienne": bool(synthesis.get("guide_utilisation_quotidienne"))
            },
            "validation": "CONFORME USAGE M√âDICAL PROFESSIONNEL"
        }
    
    def cleanup_temp_files(self, pdf_chunks: List[Path], prepared_pdf: Path, original_pdf: Path):
        """Nettoyage strict des fichiers temporaires"""
        logger.info("üßπ Nettoyage des fichiers temporaires...")
        
        # Nettoyer les chunks PDF
        if pdf_chunks and len(pdf_chunks) > 1:
            for chunk_file in pdf_chunks:
                try:
                    if chunk_file.exists():
                        chunk_file.unlink()
                        logger.debug(f"‚úÖ Chunk supprim√©: {chunk_file.name}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Impossible de supprimer {chunk_file.name}: {e}")
            
            # Supprimer le dossier de chunks
            try:
                chunk_dir = pdf_chunks[0].parent
                if chunk_dir.exists() and not any(chunk_dir.iterdir()):
                    chunk_dir.rmdir()
                    logger.info(f"‚úÖ Dossier chunks supprim√©: {chunk_dir.name}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de supprimer le dossier chunks: {e}")
        
        # Nettoyer le PDF d√©chiffr√© temporaire
        if prepared_pdf != original_pdf and prepared_pdf.exists():
            try:
                prepared_pdf.unlink()
                logger.info(f"‚úÖ PDF d√©chiffr√© temporaire supprim√©: {prepared_pdf.name}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de supprimer le PDF d√©chiffr√©: {e}")
        
        logger.info("‚úÖ Nettoyage termin√©")
    
    def emergency_cleanup(self, pdf_chunks: List[Path], prepared_pdf: Path, original_pdf: Path):
        """Nettoyage d'urgence en cas d'erreur"""
        logger.info("üö® Nettoyage d'urgence en cours...")
        
        try:
            self.cleanup_temp_files(pdf_chunks, prepared_pdf, original_pdf)
        except:
            logger.warning("‚ö†Ô∏è Nettoyage d'urgence partiel seulement")


def main():
    """Fonction principale STRICTE"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Lab Manual Analyzer - Version M√©dicale Stricte avec Double Validation")
    parser.add_argument("input_path", help="Chemin vers le fichier PDF m√©dical")
    parser.add_argument("--config", default="config.json", help="Fichier de configuration")
    
    args = parser.parse_args()
    
    try:
        # Initialisation STRICTE
        analyzer = LabManualAnalyzerStrict(args.config)
        
        input_path = Path(args.input_path)
        
        if not input_path.is_file() or input_path.suffix.lower() != '.pdf':
            print(f"‚ùå ERREUR: {input_path} n'est pas un fichier PDF valide")
            print(f"Usage: python {Path(__file__).name} chemin/vers/manuel_medical.pdf")
            return 1
        
        print(f"üè• ANALYSE M√âDICALE STRICTE AVEC DOUBLE VALIDATION: {input_path.name}")
        print(f"üìÑ PDF LaTeX sera g√©n√©r√© dans: {analyzer.syntheses_dir}")
        print("‚è≥ Traitement en cours (mode strict avec Gemini 2.0 Flash + correction automatique)...")
        
        # Analyse STRICTE
        result = analyzer.analyze_manual_organized(input_path)
        
        if result.get("success"):
            print(f"\n‚úÖ ANALYSE M√âDICALE R√âUSSIE AVEC DOUBLE VALIDATION")
            print(f"üìÑ PDF M√âDICAL G√âN√âR√â: {result['pdf_medical']}")
            
            print(f"\nüìä STATISTIQUES:")
            stats = result.get("statistiques", {})
            for key, value in stats.items():
                if isinstance(value, dict):
                    print(f"   {key.replace('_', ' ').title()}:")
                    for subkey, subvalue in value.items():
                        print(f"     - {subkey.replace('_', ' ')}: {subvalue}")
                else:
                    print(f"   {key.replace('_', ' ').title()}: {value}")
            
            print(f"\nüî¨ VALIDATION: {result.get('validation')}")
            
            synthesis = result.get("synthesis", {})
            if synthesis:
                procedures = synthesis.get("procedures_analyses", [])
                maintenance = synthesis.get("maintenance_preventive", [])
                print(f"\nüìã CONTENU M√âDICAL EXTRAIT AVEC DOUBLE VALIDATION:")
                print(f"   üß™ {len(procedures)} proc√©dures d'analyse m√©dicales d√©taill√©es")
                print(f"   üîß {len(maintenance)} proc√©dures de maintenance pr√©ventive")
                print(f"   üìä Guide d'utilisation quotidienne: {'‚úÖ' if synthesis.get('guide_utilisation_quotidienne') else '‚ùå'}")
            
            print(f"\nüéØ FICHIER FINAL: {result['pdf_medical']}")
            print(f"   ‚û§ Document PDF professionnel pr√™t pour usage m√©dical")
            print(f"   ‚û§ Extraction maximale d'informations avec validation JSON")
            print(f"   ‚û§ Compatible Gemini 2.0 Flash + correction automatique")
            
        else:
            print(f"\n‚ùå √âCHEC DE L'ANALYSE M√âDICALE")
            print(f"üö® ERREUR CRITIQUE: {result.get('error')}")
            print(f"\nüîç DIAGNOSTIC:")
            print(f"   1. V√©rifiez la configuration dans config.json")
            print(f"   2. Contr√¥lez vos quotas Google Cloud et Gemini")
            print(f"   3. Assurez-vous que LaTeX est install√©")
            print(f"   4. Consultez les logs: lab_analysis.log")
            print(f"\n‚ö†Ô∏è  AUCUN FICHIER G√âN√âR√â - S√©curit√© m√©dicale respect√©e")
            return 1
    
    except Exception as e:
        print(f"\nüí• ERREUR CRITIQUE SYST√àME: {e}")
        logger.error(f"Erreur critique dans main: {e}")
        print(f"\nüö® ANALYSE INTERROMPUE - Aucun fichier g√©n√©r√©")
        print(f"   Ceci est normal en mode m√©dical strict")
        print(f"   Consultez lab_analysis.log pour diagnostic complet")
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)